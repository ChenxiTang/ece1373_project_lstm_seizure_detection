# -*- coding: utf-8 -*-
"""Untitled15.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lFKLYY5-tS84-UGZZASbNa0_iBxytmpS
"""

from google.colab import drive
drive.mount('/content/gdrive')
import numpy as np
import tensorflow as tf
from tensorflow.contrib import rnn 

y_train = np.load('/content/gdrive/My Drive/data/y_train.npy')
y_val = np.load('/content/gdrive/My Drive/data/y_test.npy')
x_train = np.load('/content/gdrive/My Drive/data/x_train_se.npy')
x_val = np.load('/content/gdrive/My Drive/data/x_test_se.npy')


truncated_backprop_length = 5*256

#x_train = np.reshape(x_train,(y_train.shape[0],-1,x_train.shape[1]))
#x_val = np.reshape(x_val,(y_val.shape[0],-1,x_val.shape[1]))

y_train = np.reshape(y_train,(-1,truncated_backprop_length))
y_val = np.reshape(y_val,(-1,truncated_backprop_length))
x_train = np.reshape(x_train,(-1,truncated_backprop_length,x_train.shape[1]))
x_val = np.reshape(x_val,(-1,truncated_backprop_length,x_val.shape[1]))

print(y_train.shape)
print(y_val.shape)
print(x_train.shape)
print(x_val.shape)


batch_size_train = x_train.shape[0]
batch_size_val = x_val.shape[0]


num_epochs = 500
state_size = 64
num_classes = 2
feature_size = 22*5

window_size = 5*256
num_batches_train = x_train.shape[1]//truncated_backprop_length
num_batches_val = x_val.shape[1]//truncated_backprop_length

tf.reset_default_graph()

batchX_placeholder = tf.placeholder(tf.float32, [None, truncated_backprop_length, feature_size])
batchY_placeholder = tf.placeholder(tf.float32, [None,truncated_backprop_length,1])

cell_state = tf.placeholder(tf.float32, [None, state_size])
hidden_state = tf.placeholder(tf.float32, [None, state_size])
init_state = tf.nn.rnn_cell.LSTMStateTuple(cell_state, hidden_state)
#W2 = tf.Variable(np.random.rand(state_size*truncated_backprop_length, num_classes-1),dtype=tf.float32)
W2 = tf.Variable(np.random.rand(state_size, num_classes-1),dtype=tf.float32)
b2 = tf.Variable(np.zeros((1,num_classes-1)), dtype=tf.float32)

# Unpack columns
inputs_series = tf.unstack(batchX_placeholder, axis=1)
labels_series = tf.unstack(batchY_placeholder, axis=1)

"""
logits_series = [tf.matmul(state, W2) + b2 for state in states_series] #Broadcasted addition
predictions_series = [tf.nn.softmax(logits) for logits in logits_series]

losses = [tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels) for logits, labels in zip(logits_series,labels_series)]
total_loss = tf.reduce_mean(losses)
"""
# Forward passes
cell = rnn.BasicLSTMCell(state_size)
states_series, current_state = rnn.static_rnn(cell, inputs_series, init_state,dtype=tf.float32)
#states_concat = tf.concat(states_series, 1)
#logits_series = tf.matmul(states_concat, W2) + b2 #Broadcasted addition
logits_series = [tf.matmul(state, W2) + b2 for state in states_series] #Broadcasted addition
#predictions_series=tf.nn.sigmoid(logits_series)
predictions_series = [tf.nn.sigmoid(logits) for logits in logits_series]
#losses = tf.nn.sigmoid_cross_entropy_with_logits(logits = logits_series, labels = batchY_placeholder)
#print(tf.shape(logits_series))
losses = [tf.nn.sigmoid_cross_entropy_with_logits(logits = logit, labels = label) for logit, label in zip(logits_series,labels_series)]
total_loss = tf.reduce_mean(losses)
#train_step = tf.train.AdagradOptimizer(0.3).minimize(total_loss)
train_step = tf.train.AdamOptimizer().minimize(total_loss)
#model evaluation
#correct_prediction=[tf.equal(tf.round(predictions_series),batchY_placeholder)]
correct_prediction = [tf.equal(tf.round(prediction),label) for prediction, label in zip(predictions_series,labels_series)]
#rounded_prediction=[tf.round(prediction) for prediction in predictions_series]

accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))




# Training

with tf.Session() as sess:
    sess.run(tf.initialize_all_variables())
    loss_list = []
    early_stopping_counter = 0
    max_val_accuracy = 0
    for epoch_idx in range(num_epochs):

        _current_cell_state = np.zeros((batch_size_train, state_size))
        _current_hidden_state = np.zeros((batch_size_train, state_size))

        print("New data, epoch", epoch_idx)

        for batch_idx in range(num_batches_train):
            start_idx = batch_idx * truncated_backprop_length
            end_idx = start_idx + truncated_backprop_length

            batchX = x_train[:,start_idx:end_idx,:]
            batchY = y_train[:,start_idx:end_idx]
            batchY = np.reshape(batchY,(-1,truncated_backprop_length,1))  
            #batchY = np.reshape(batchY,(-1,1))

            #if batch_idx%(window_size//truncated_backprop_length) == 0:
            #    _current_cell_state = np.zeros((batch_size_train, state_size))
            #    _current_hidden_state = np.zeros((batch_size_train, state_size))
            #    #print("set states to 0")

            _total_loss, _train_step, _current_state, _predictions_series = sess.run(
                [total_loss, train_step, current_state, predictions_series],
                 feed_dict={
                 batchX_placeholder: batchX,
                 batchY_placeholder: batchY,
                 cell_state: _current_cell_state,
                 hidden_state: _current_hidden_state
                 })

            _current_cell_state, _current_hidden_state = _current_state

            loss_list.append(_total_loss)

            #if batch_idx%100 == 0:
            print("Step",batch_idx, "Batch loss", _total_loss)


        _current_cell_state = np.zeros((batch_size_val, state_size))
        _current_hidden_state = np.zeros((batch_size_val, state_size))
        print("starting validation")
        val_accuracy = 0
        #auc_value = 0
        #label_values = []   #maxed over 6 subwindows
        #val_values = []     #maxed over 6 subwindows
        #auc_values = []     #maxed over 6 subwindows
        for batch_idx in range(num_batches_val):
            start_idx = batch_idx * truncated_backprop_length
            end_idx = start_idx + truncated_backprop_length

            batchX = x_val[:,start_idx:end_idx,:]
            batchY = y_val[:,start_idx:end_idx]
            batchY = np.reshape(batchY,(-1,truncated_backprop_length,1))   
            #print("here1")
            #if batch_idx%(window_size//truncated_backprop_length) == 0:
            #    _current_cell_state = np.zeros((batch_size_val, state_size))
            #    _current_hidden_state = np.zeros((batch_size_val, state_size))
                #print("set states to 0 in val")

            _accuracy, _current_state, _predictions_series=sess.run(
                [accuracy, current_state, predictions_series],
                feed_dict={
                    batchX_placeholder: batchX,
                    batchY_placeholder: batchY,
                    cell_state: _current_cell_state,
                    hidden_state: _current_hidden_state
            })
            #print("here2")
            _current_cell_state, _current_hidden_state = _current_state
            #print(tf.shape(_rounded_prediction))
            #label_values.append(batchY)
            #val_values.append(_rounded_prediction)
            #auc_values.append(_predictions_series)
            #val_accuracy+=_accuracy
            #auc_value += roc_auc_score(batchY, _predictions_series)
            #print("here3")
            val_accuracy = _accuracy
            #print("here4")

        #val_accuracy = val_accuracy/num_batches_val
        #auc_value = auc_value/num_batches_val
        """
        label_values = np.concatenate(label_values,axis=1)
        val_values = np.concatenate(val_values,axis=1)
        auc_values = np.concatenate(auc_values,axis=1)

        how_many = num_batches_val//6
        label_values = np.split(label_values,how_many,axis=1)
        label_values = [x.max(axis=1) for x in label_values]

        val_values = np.split(val_values,how_many,axis=1)
        val_values = [x.max(axis=1) for x in val_values]

        auc_values = np.split(auc_values,how_many,axis=1)
        auc_values = [x.max(axis=1) for x in auc_values]

        label_values = np.concatenate(label_values,axis=0)
        val_values = np.concatenate(val_values,axis=0)
        auc_values = np.concatenate(auc_values,axis=0)

        val_accuracy = np.mean(label_values == val_values) # maxed over 6 subwindows
        auc_value = roc_auc_score(label_values, auc_values) # maxed over 6 subwindows
        """
        
        if epoch_idx == 0:
            max_val_accuracy = val_accuracy
        else:
            if val_accuracy > max_val_accuracy:
                max_val_accuracy = val_accuracy
                early_stopping_counter = 0
            else:
                early_stopping_counter += 1

        if early_stopping_counter >= 20:
            break


        #print("Current Val Accuracy, Max Val Accuracy, Current AUC:", val_accuracy, max_val_accuracy, auc_value)
        print("Current Val Accuracy, Max Val Accuracy:", val_accuracy, max_val_accuracy)